<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>贝叶斯个性化推荐</title>
    <url>/uncategorized/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90/</url>
    <content><![CDATA[<body>
<div>
<p>　　　　在<a id="post_title_link_6351319" href="https://www.cnblogs.com/pinard/p/6351319.html" target="_blank" rel="noopener">矩阵分解在协同过滤推荐算法中的应用</a>中，我们讨论过像funkSVD之类的矩阵分解方法如何用于推荐。今天我们讲另一种在实际产品中用的比较多的推荐算法:贝叶斯个性化排序(Bayesian Personalized Ranking, 以下简称BPR)，它也用到了矩阵分解，但是和funkSVD家族却有很多不同之处。下面我们来详细讨论。</p>
<h1>1.&nbsp; BPR算法使用背景</h1>
<p>　　　　在很多推荐场景中，我们都是基于现有的用户和商品之间的一些数据，得到用户对所有商品的评分，选择高分的商品推荐给用户，这是funkSVD之类算法的做法，使用起来也很有效。但是在有些推荐场景中，我们是为了在千万级别的商品中推荐个位数的商品给用户，此时，我们更关心的是用户来说，哪些极少数商品在用户心中有更高的优先级，也就是排序更靠前。也就是说，我们需要一个排序算法，这个算法可以把每个用户对应的所有商品按喜好排序。BPR就是这样的一个我们需要的排序算法。</p>
<h1>2.&nbsp; 排序推荐算法背景介绍</h1>
<p>　　　　&nbsp;排序推荐算法历史很悠久，早在做信息检索的各种产品中就已经在使用了。最早的第一类排序算法类别是点对方法(Pointwise Approach)，这类算法将排序问题被转化为分类、回归之类的问题，并使用现有分类、回归等方法进行实现。第二类排序算法是成对方法(Pairwise Approach)，在序列方法中，排序被转化为对序列分类或对序列回归。所谓的pair就是成对的排序，比如(a,b)一组表明a比b排的靠前。我们要讲到的BPR就属于这一类。第三类排序算法是列表方法(Listwise&nbsp;Approach)，它采用更加直接的方法对排序问题进行了处理。它在学习和预测过程中都将排序列表作为一个样本。排序的组结构被保持。</p>
<p>　　　　本文关注BPR，这里我们对排序推荐算法本身不多讲，如果大家感兴趣，可以阅读李航的<a title="A Short Introduction to Learning to Rank" href="http://times.cs.uiuc.edu/course/598f13/l2r.pdf" target="_blank">A Short Introduction to Learning to Rank</a>.</p>
<h1>3. BPR建模思路</h1>
<p>　　　　在BPR算法中，我们将任意用户u对应的物品进行标记，如果用户u在同时有物品i和j的时候点击了i，那么我们就得到了一个三元组$&lt;u,i,j&gt;$，它表示对用户u来说，i的排序要比j靠前。如果对于用户u来说我们有m组这样的反馈，那么我们就可以得到m组用户u对应的训练样本。</p>
<p>　　　　既然是基于贝叶斯，那么我们也就有假设，这里的假设有两个：一是每个用户之间的偏好行为相互独立，即用户u在商品i和j之间的偏好和其他用户无关。二是同一用户对不同物品的偏序相互独立，也就是用户u在商品i和j之间的偏好和其他的商品无关。为了便于表述，我们用$&gt;_u$符号表示用户u的偏好，上面的$&lt;u,i,j&gt;$可以表示为：$i&gt;_uj$。</p>
<p>　　　　在BPR中，这个排序关系符号$&gt;_u$满足完全性，反对称性和传递性，即对于用户集U和物品集I：</p>
<p>　　　　完整性：$\forall i,j \in I: i \neq j \Rightarrow i &gt;_u j\; \cup\;&nbsp; j&gt;_u i$</p>
<p>　　　　反对称性：$\forall i,j \in I:&nbsp;i &gt;_u j\; \cap\;&nbsp; j&gt;_u i \Rightarrow i=j$</p>
<p>　　　　传递性：$\forall i,j,k \in I:&nbsp;i &gt;_u j\; \cap\;&nbsp; j&gt;_u k&nbsp;\Rightarrow i&gt;_uk$</p>
<p>　　　　同时，BPR也用了和funkSVD类似的矩阵分解模型，这里BPR对于用户集U和物品集I的对应的$U \times I$的预测排序矩阵$\overline{X}$，我们期望得到两个分解后的用户矩阵$W$($|U| \times k$)和物品矩阵$H$($|I| \times k$)，满足$$\overline{X} = WH^T$$</p>
<p>　　　　这里的k和funkSVD类似，也是自己定义的，一般远远小于$|U|,|I|$。</p>
<p>　　　　由于BPR是基于用户维度的，所以对于任意一个用户u，对应的任意一个物品i我们期望有：$$\overline{x}_{ui} = w_u \bullet h_i = \sum\limits_{f=1}^kw_{uf}h_{if}$$</p>
<p>　　　　最终我们的目标，是希望寻找合适的矩阵$W,H$，让$\overline{X}$和$X$最相似。读到这里，也许你会说，这和funkSVD之类的矩阵分解模型没有什么区别啊？ 的确，现在还看不出，下面我们来看看BPR的算法优化思路，就会慢慢理解和funkSVD有什么不同了。</p>
<h1>4. BPR的算法优化思路</h1>
<p>　　　　BPR 基于最大后验估计$P(W,H|&gt;_u)$来求解模型参数$W,H$,这里我们用$\theta$来表示参数$W$和$H$, $&gt;_u$代表用户u对应的所有商品的全序关系,则优化目标是$P(\theta|&gt;_u)$。根据贝叶斯公式，我们有：$$P(\theta|&gt;_u) = \frac{P(&gt;_u|\theta)P(\theta)}{P(&gt;_u)}$$</p>
<p>　　　　由于我们求解假设了用户的排序和其他用户无关，那么对于任意一个用户u来说，$P(&gt;_u)$对所有的物品一样，所以有：$$P(\theta|&gt;_u) \propto P(&gt;_u|\theta)P(\theta)$$</p>
<p>　　　　这个优化目标转化为两部分。第一部分和样本数据集D有关，第二部分和样本数据集D无关。</p>
<p>　　　　对于第一部分，由于我们假设每个用户之间的偏好行为相互独立，同一用户对不同物品的偏序相互独立，所以有：$$\prod_{u \in U}P(&gt;_u|\theta) = \prod_{(u,i,j) \in (U \times I \times I)}P(i &gt;_u j|\theta)^{\delta((u,i,j) \in D)}(1-P(i &gt;_u j|\theta))^{\delta((u,j,i) \not\in D) }$$</p>
<p>　　　　其中，$$\delta(b)= \begin{cases} 1&amp; {if\; b\; is \;true}\\ 0&amp; {else} \end{cases}$$　　　</p>
<p>　　　　根据上面讲到的完整性和反对称性，优化目标的第一部分可以简化为：$$\prod_{u \in U}P(&gt;_u|\theta) = \prod_{(u,i,j) \in D}P(i &gt;_u j|\theta)$$</p>
<p>　　　　而对于$P(i &gt;_u j|\theta)$这个概率，我们可以使用下面这个式子来代替:$$P(i &gt;_u j|\theta) = \sigma(\overline{x}_{uij}(\theta))$$</p>
<p>　　　　其中，$\sigma(x)$是sigmoid函数。这里你也许会问，为什么可以用这个sigmoid函数来代替呢? 其实这里的代替可以选择其他的函数，不过式子需要满足BPR的完整性，反对称性和传递性。原论文作者这么做除了是满足这三个性质外，另一个原因是为了方便优化计算。</p>
<p>　　　　对于$\overline{x}_{uij}(\theta)$这个式子，我们要满足当$i &gt;_u j$时，$\overline{x}_{uij}(\theta) &gt; 0$, 反之当$j &gt;_u i$时，$\overline{x}_{uij}(\theta) &lt; 0$，最简单的表示这个性质的方法就是$$\overline{x}_{uij}(\theta) = \overline{x}_{ui}(\theta) - \overline{x}_{uj}(\theta)$$</p>
<p>　　　　而$\overline{x}_{ui}(\theta) , \overline{x}_{uj}(\theta)$，就是我们的矩阵$ \overline{X}$对应位置的值。这里为了方便，我们不写$\theta$,这样上式可以表示为:$$\overline{x}_{uij} = \overline{x}_{ui} - \overline{x}_{uj}$$</p>
<p>　　　　注意上面的这个式子也不是唯一的，只要可以满足上面提到的当$i &gt;_u j$时，$\overline{x}_{uij}(\theta) &gt; 0$，以及对应的相反条件即可。这里我们仍然按原论文的式子来。</p>
<p>　　　　最终，我们的第一部分优化目标转化为：$$\prod_{u \in U}P(&gt;_u|\theta) = \prod_{(u,i,j) \in D} \sigma(\overline{x}_{ui} - \overline{x}_{uj})$$　　　　</p>
<p>　　　　对于第二部分$P(\theta)$，原作者大胆使用了贝叶斯假设，即这个概率分布符合正太分布，且对应的均值是0，协方差矩阵是$\lambda_{\theta}I$,即$$P(\theta) \sim N(0, \lambda_{\theta}I)$$</p>
<p>　　　　原作者为什么这么假设呢？个人觉得还是为了优化方便，因为后面我们做优化时，需要计算$lnP(\theta) $，而对于上面假设的这个多维正态分布，其对数和$||\theta||^2$成正比。即：$$lnP(\theta) = \lambda||\theta||^2$$</p>
<p>　　　　最终对于我们的最大对数后验估计函数$ln\;P(\theta|&gt;_u) \propto ln\;P(&gt;_u|\theta)P(\theta) = ln\;\prod\limits_{(u,i,j) \in D} \sigma(\overline{x}_{ui} - \overline{x}_{uj}) + ln P(\theta) = \sum\limits_{(u,i,j) \in D}ln\sigma(\overline{x}_{ui} - \overline{x}_{uj}) + \lambda||\theta||^2\;$　　　</p>
<p>　　　　这个式子可以用梯度上升法或者牛顿法等方法来优化求解模型参数。如果用梯度上升法，对$\theta$求导，我们有： $$\frac{\partial ln\;P(\theta|&gt;_u)}{\partial \theta} \propto \sum\limits_{(u,i,j) \in D} \frac{1}{1+e^{\overline{x}_{ui} - \overline{x}_{uj}}}\frac{\partial (\overline{x}_{ui} - \overline{x}_{uj})}{\partial \theta} + \lambda \theta$$</p>
<p>　　　　由于$$\overline{x}_{ui} - \overline{x}_{uj} = \sum\limits_{f=1}^kw_{uf}h_{if} - \sum\limits_{f=1}^kw_{uf}h_{jf}$$</p>
<p>　　　　这样我们可以求出：$$\frac{\partial (\overline{x}_{ui} - \overline{x}_{uj})}{\partial \theta} = \begin{cases} (h_{if}-h_{jf})&amp; {if\; \theta = w_{uf}}\\ w_{uf}&amp; {if\;\theta = h_{if}} \\ -w_{uf}&amp; {if\;\theta = h_{jf}}\end{cases}$$　</p>
<p>　　　　有了梯度迭代式子，用梯度上升法求解模型参数就容易了。下面我们归纳下BPR的算法流程。</p>
<h1>5. BPR算法流程</h1>
<p>　　　　下面简要总结下BPR的算法训练流程：　　</p>
<p>　　　　输入：训练集D三元组，梯度步长$\alpha$， 正则化参数$\lambda$,分解矩阵维度k。　　　　　　　　　　</p>
<p>　　　　输出：模型参数，矩阵$W,H$</p>
<p>　　　　1. 随机初始化矩阵$W,H$</p>
<p>　　　　2. 迭代更新模型参数：</p>
<p>$$w_{uf} =w_{uf} + \alpha(\sum\limits_{(u,i,j) \in D} \frac{1}{1+e^{\overline{x}_{ui} - \overline{x}_{uj}}}(h_{if}-h_{jf}) + \lambda w_{uf}) $$</p>
<p>$$h_{if} =h_{if} + \alpha(\sum\limits_{(u,i,j) \in D} \frac{1}{1+e^{\overline{x}_{ui} - \overline{x}_{uj}}}w_{uf} + \lambda h_{if}) $$</p>
<p>$$h_{jf} =h_{jf} + \alpha(\sum\limits_{(u,i,j) \in D} \frac{1}{1+e^{\overline{x}_{ui} - \overline{x}_{uj}}}(-w_{uf}) + \lambda h_{jf}) $$</p>
<p>　　　　3. 如果$W,H$收敛，则算法结束，输出W,H，否则回到步骤2.</p>
<p>　　　　当我们拿到$W,H$后，就可以计算出每一个用户u对应的任意一个商品的排序分：$\overline{x}_{ui} = w_u \bullet h_i $，最终选择排序分最高的若干商品输出。</p>
<h1>6. BPR小结</h1>
<p>　　　　BPR是基于矩阵分解的一种排序算法，但是和funkSVD之类的算法比，它不是做全局的评分优化，而是针对每一个用户自己的商品喜好分贝做排序优化。因此在迭代优化的思路上完全不同。同时对于训练集的要求也是不一样的，funkSVD只需要用户物品对应评分数据二元组做训练集，而BPR则需要用户对商品的喜好排序三元组做训练集。</p>
<p>　　　　在实际产品中，BPR之类的推荐排序在海量数据中选择极少量数据做推荐的时候有优势，因此在某宝某东等大厂中应用也很广泛。由于BPR并不复杂，下一篇我会用tensorflow来做一个BPR的实践，敬请期待。</p>
</div>
</body>
]]></content>
      <tags>
        <tag>推荐算法</tag>
      </tags>
  </entry>
  <entry>
    <title>流形学习之简介</title>
    <url>/uncategorized/%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<ol>
<li>记得前几天在汇报时，讲到LLE降维算法时，洋洋洒洒讲了一大堆公式推导，之后老师问了一个很实在的问题：什么是流形，当时支支吾吾说了半天，都没说到点上，后来查阅了一些相关资料，现在总结如下。</li>
<li><strong>前面那篇文章已经介绍过如何用主成分分析降维——它可以在减少数据集特征的同时，保留数据点间的必要关系。虽然 PCA 是一个灵活、快速且容易解释的算法，但是它对存在非线性关系的数据集的处理效果并不太好，我们将在后面介绍几个示例。为了弥补这个缺陷，我们选择另外一种方法——流形学习（manifold learning）。流形学习是一种无监督评估器，它试图将一个低维度流形嵌入到一个高维度空间来描述数据集。当你思考流形时，建议你设想有一张纸——一个存在于我们所熟悉的三维世界中的二维物体——它可以从两个维度弯折或卷起。提到流形学习这个术语时，可以把这张纸看成那个嵌入三维空间中二维流形。在三维空间中旋转、重定向或者伸展这张纸，都不会改变它的平面几何特性：这些操作和线性嵌入类似。如果你弯折、卷曲或者弄皱这张纸，它仍然是一个二维流形，但是嵌入到一个三维空间就不再是线性的了。流形学习算法将试图学习这张纸的二维特征，包括将纸弯曲后放入一个三维空间中。</strong></li>
<li><p>为了说明这个情况，我们生成一个hello形状的图片，之后将一些点填充进去，得到如下的图：</p>
<pre><code> ![在这里插入图片描述](https://img-blog.csdnimg.cn/20190623145652857.png?)
输出图像包含了很多二维的点，它们组成了单词“HELLO”的形状。这个数据形状可以帮助我们通过可视化的方式展现算法的使用过程。
3.1 首先看一下多维标度法（MDS）：通过观察这个数据集，可以看到数据集中选中的 x 值和 y 值并不是对数据的最基本描述：即使放大、缩小或旋转数据，“HELLO”仍然会很明显。例如，如果用一个旋转矩阵来旋转数据，x 和 y 的值将会改变，但是数据形状基本还是一样的（如下图所示）：
    ![在这里插入图片描述](https://img-blog.csdnimg.cn/20190623145954540.png?)
这说明 x 和 y 的值并不是数据间关系的必要基础特征。这个例子中真正的基础特征是每个点与数据集中其他点的距离。表示这种关系的常用方法是关系（距离）矩阵：对于 N 个点，构建一个 N × N 的矩阵，元素 (i, j ) 是点 i 和点 j 之间的距离。对于 N = 1000 个点，获得了一个 1000 × 1000 的矩阵。画出该矩阵，如下图所示：
    ![在这里插入图片描述](https://img-blog.csdnimg.cn/20190623150157794.png?)
如果用类似方法为已经做过旋转和平移变换的数据构建一个距离矩阵，将看到同样的结果。这个距离矩阵给出了一个数据集内部关系的表现形式，这种形式与数据集的旋转和投影无关。但距离矩阵的可视化效果却显得不够直观。上图丢失了我们之前在数据中看到的关于“HELLO”的所有视觉特征。虽然从 (x, y) 坐标计算这个距离矩阵很简单，但是从距离矩阵转换回x 坐标值和 y 坐标值却非常困难。这就是多维标度法（MDS）可以解决的问题：它可以将一个数据集的距离矩阵还原成一个 D 维坐标来表示数据集。仅仅依靠描述数据点间关系的 N × N 距离矩阵，MDS 算法就可以为数据还原出一种可行的二维坐标。还原后的效果如图所示：
</code></pre><p><img src="https://img-blog.csdnimg.cn/20190623150503210.png?" alt="在这里插入图片描述"></p>
<pre><code>既然距离矩阵可以从数据的任意维度进行计算，那么这种方法绝对非常实用。既然可以在一个二维平面中简单地旋转数据，那么也可以将其投影到三维空间。效果如下图：
    ![在这里插入图片描述](https://img-blog.csdnimg.cn/20190623150810136.png?)
现在可以通过 MDS 评估器输入这个三维数据，计算距离矩阵，然后得出距离矩阵的最优二维嵌入结果。结果还原了原始数据的形状，如下图：
    ![在这里插入图片描述](https://img-blog.csdnimg.cn/20190623150908545.png?)
以上就是使用流形学习评估器希望达成的基本目标：给定一个高维嵌入数据，寻找数据的一个低维表示，并保留数据间的特定关系。在 MDS的示例中，保留的数据是每对数据点之间的距离。
</code></pre></li>
<li><p>非线性嵌入：当MDS失败时：</p>
<pre><code>前面介绍了线性嵌入模型，它包括将数据旋转、平移和缩放到一个高维空间的操作。但是当嵌入为非线性时，即超越简单的操作集合时，MDS算法就会失效。现在看看下面这个将输入数据在三维空间中扭曲成“S”形状的示例：        
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190623151215112.png?)
虽然数据点间基本的关系仍然存在，但是这次数据以非线性的方式进行了变换：它被包裹成了“S”形。如果尝试用一个简单的 MDS 算法来处理这个数据，就无法展示数据非线性嵌入的特征，进而导致我们丢失了这个嵌入式流形的内部基本关系特性，即使是最优的二维线性嵌入也不能破解 S 曲线的谜题，而且还丢失了原始数据的 y 轴信息。如下图所示：
![在这里插入图片描述](https://img-blog.csdnimg.cn/2019062315134030.png?)
**非线性流形：局部线性嵌入**
</code></pre><p>那么该如何改进呢？在学习新的内容之前，先来回顾一下问题的源头：MDS 算法构建嵌入时，总是期望保留相距很远的数据点之间的距离。但是如果修改算法，让它只保留比较接近的点之间的距离呢？嵌入的结果可能会与我们的期望更接近。<br>可以将这两种思路想象成图 所示的情况。(MDS和LLE表示点间距离的差异)<br><img src="https://img-blog.csdnimg.cn/20190623151541691.png?" alt="在这里插入图片描述"><br>其中每一条细小的线都表示在嵌入时会保留的距离。左图是用 MDS 算法生成的嵌入模型，它会试图保留数据集中每对数据点间的距离；右图是用流形学习算法局部线性嵌入（LLE）生成的嵌入模型，该方法不保留所有的距离，而是仅保留邻节点间的距离——本例选择与每个点最近的 100 个邻节点。看看左图，就能够明白为什么 MDS 算法会失效了：显然不可能在展开数据的同时，保证每条线段的长度完全不变。相比之下，右图的情况就更乐观一些。我们可以想象着通过某种方式将卷曲的数据展开，并且线段的长度基本保持不变。这就是 LLE 算法的工作原理，它通过对成本函数的全局优化来反映这个逻辑。LLE 有好几种表现形式，这里用 modified LLE 算法来还原嵌入的二维流形。通常情况下，modified LLE 的效果比用其他算法还原实现定义好的流形数据的效果好，它几乎不会造成扭曲，如图所示：</p>
<pre><code>![在这里插入图片描述](https://img-blog.csdnimg.cn/20190623151811661.png?)
</code></pre><p>比起原始流形，这个结果虽然出现了一定程度的扭曲，但还是保留了数据的基本关系特性。</p>
</li>
<li><p>类似的方法还有拉普拉斯特征映射。核心思想也是在流形中相近的结点我们希望它降维之后也越相近，上面都是从直观上理解各种算法原理，实际还有很复杂的公式推导。不论是LLE还是拉普拉斯特征映射，它们都是强调在原空间距离相近的点降维后也越接近，但没有说明在原空间较远的点降维后有什么限制条件，这样通常都会导致经过降维后虽然相近的点距离足够相近，但原先距离相近的点和原先距离较远的点往往没有明显的界限，没有很好的区分性。以手写数字为例：经过降维后常常得到如下的结果：</p>
<pre><code> ![在这里插入图片描述](https://img-blog.csdnimg.cn/20190623152811390.png?)
</code></pre><p>基于这种问题，引入了t-SNE算法。不像之前一样使用距离，而是计算原空间和降维后空间各个点的相似度，用KL散度来描述前后空间分布的相似度：</p>
<pre><code> $$P\left(x^{j} | x^{i}\right)=\frac{S\left(x^{i}, x^{j}\right)}{\sum_{k \neq i} S\left(x^{i}, x^{k}\right)} \qquad Q\left(z^{j} | z^{i}\right)=\frac{S^{\prime}\left(z^{i}, z^{j}\right)}{\sum_{k \neq i} S^{\prime}\left(z^{i}, z^{k}\right)}$$
 $$L=\sum_{i} K L\left(P\left(* | x^{i}\right) \| Q\left(* | z^{i}\right)\right)$$
 $$ \quad \quad  =\sum_{i} \sum_{j} P\left(x^{j} | x^{i}\right) \log \frac{P\left(x^{j} | x^{i}\right)}{Q\left(z^{j} | z^{i}\right)}$$
</code></pre><p>计算原空间和目标空间相似度时，t-SNE 使用不同的计算公式：</p>
<pre><code>$$S\left(x^{i}, x^{j}\right)=\exp \left(-\left\|x^{i}-x^{j}\right\|_{2}\right)$$ $$S^{\prime}\left(z^{i}, z^{j}\right)=1 / 1+\left\|z^{i}-z^{j}\right\|_{2}$$
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190623153726840.png?)
</code></pre><p>最后的效果可以看到不仅每一个类别的数字都聚在一起，不同类别的数字也区分较明显，效果不错。<img src="https://img-blog.csdnimg.cn/20190623155221622.png?" alt="在这里插入图片描述"></p>
</li>
</ol>
<blockquote>
<p>参考资料：Python数据科学手册，台大李宏毅机器学习公开课教程。</p>
</blockquote>
]]></content>
  </entry>
</search>
